  %! TeX program = lualatex
\documentclass[12pt]{amsart}

\input{usepackages}
\input{theoremenv}
\input{title}
\input{mathdefs}

\addbibresource{ref.bib}

\begin{document}
\maketitle

\section{Background}
Many real-world routing and scheduling problems are subject to uncertainty. Take delivery service as an example, a courier has several customers located at different cities and it needs to plan a route that visits the customers. The delivery person may find that 
a household might not have packages to deliver today, or customer requests can be canceled, etc..
A classical abstraction is the stochastic
traveling salesperson problem (stochastic TSP), where each vertex $v$ comes with a specified probability $p_v$ and becomes \textit{active} (requires service) independently with probability $p_v$. The goal is to find a tour that visits all active vertices and minimizes the expected travel cost, where the cost comes from the underlying metric on vertices.

An important modeling choice is whether the salesperson is allowed to adapt their future decisions based on information revealed during the execution of the tour. In the nonadaptive (a priori) setting, the salesperson commits to a master tour that only depends on the underlying metric and activation probabilities and not the actual realizations. During the actual execution of the tour, i.e., once the set of active vertices is realized, the salesperson follows the tour while shortcutting inactive vertices. In the adaptive setting, decisions may depend on observations revealed along the way, allowing the tour to be adjusted on the fly. 

A fundamental analysis question is how much adaptivity can improve performance relative to an optimal nonadaptive tour.
A related algorithmic question is whether we can design provably good adaptive policies using limited computational resources.
Note that computing an optimal adaptive policy (or a nonadaptive master tour) for stochastic TSP is computationally hard, since the setting with $p_v = 1$ for all $v \in V$ corresponds to the classical TSP which is known to be APX-hard\cite{Karpinski2015}. 
Also, describing an optimal adaptive policy might require exponential space. 
Thus, we will be interested in designing adaptive policies that can be implemented efficiently 
and its expected cost can be provably bounded in terms of the optimal cost within some small factor $\alpha \geq 1$.  

\section{Notations and Definitions}
Let $(\{r\}\cup V,d)$ be a finite metric space on $n+1$ vertices (a depot vertex $r$ and $n$ clients) equipped with a distance function $d$. 
That is, $d$ satisfies the triangle inequality: for any three vertices $u,v,w \in V$, $d(u,v) \leq d(u,w) + d(w,v)$ holds. 
A metric is said to be \emph{symmetric} if $d(x, y) = d(y, x)$ for any $x, y$.

For classical TSP, we want to find a tour $T = (r,v_1,v_2,\ldots,v_n,r)$ that starts and ends at the depot, visits all vertices exactly once, and minimizes the total distance traversed, $c(T) := d(r,v_1) + d(v_1,v_2) + \cdots + d(v_{n-1},v_n) + d(v_n,r)$. 

In the stochastic setting, the set of active vertices is a subset $A \subseteq V$ generated by independent activations: each vertex $v \in V$ is active (i.e., $v \in A$) independently with probability $p_v$.
% It precisely models a delivery planning the delivery route for customers located at different places and each customer needs delivery with a specified probability independently.
% 
We consider three different benchmarks for stochastic TSP depending on how much the tour is allowed to depend on the realization of the set of active vertices:
\begin{enumerate}
    \item \textbf{A posteriori TSP:} %\texorpdfstring{\cite{Shmoys2008}}{}} 
        Here, the salesperson knows the set $A$ of active vertices. 
        Let $\opt(A)$ denote the cost of an optimal TSP tour that visits $\{r\} \cup A$. (Note that this is simply an instance of the classical TSP.) 
        The cost of a posteriori TSP is defined to be 
        $\optpost := \E_{A \sim \mu} [\opt(A)]$, where $\mu$ is the probability distribution over the set of active vertices. 
        Note $\mu(A) = \Pi_{v \in A}p_v \cdot \Pi_{v \notin A}(1 - p_v)$ for any $A \subseteq V$.  
        
    \item \textbf{A priori (nonadaptive) TSP:} 
        Here, the salesperson only knows the metric and the activation probabilities and they are required to compute a master tour $T = (r, v_1,\ldots,v_n,r)$ that visits all vertices while starting and ending at $r$. 
        For any set $A$ of active vertices, the tour $T\vert_A$ traversed by the salesperson (for that $A$) is obtained by shortcutting $T$ to include only vertices in $A$. 
        More precisely, if $1 \leq i_1 < i_2 < \cdots < i_{|A|} \leq n$ are such that $\{v_{i_k} : 1 \leq k \leq |A|\} = A$, then $T\vert_A = (r,v_{i_1},v_{i_2},\ldots,v_{i_{|A|}},r)$.  
        We overload the notation $c$ and define the cost of $T$ to be $\E_{A \sim \mu} [c(T\vert_A)]$. 
        We define $\optprior$ to be the cost of an optimal master tour.  
        
    \item \textbf{Adaptive TSP:} 
        In this case the salesperson knows all the activation probabilities but not the realization of the set of active vertices $A$. 
        However, on the day of the execution of the tour, the salesperson calls all the customers one by one in some order. 
        If the customer $v$ that they call is in $A$, then the salesperson \emph{must} visit $v$ right away from their current position before calling any other customer. 
        On the other hand, if $v$ is not in $A$, then the salesperson remains at the current position. 
        In an adaptive policy, the salesperson is allowed to use the revealed information about the customers that they have called to decide which customer to call next.    
        
        Formally, the decision tree of an adaptive policy can be represented as a function $\pi: (\{r\} \cup V) \times 2^V \rightarrow \{r\} \cup V \cup \{\bot\}$, where $\pi(u,S)$ denotes the vertex that should be called next given that the salesperson is currently at $u$ and the set of vertices that have never been called is exactly $S$. 
        We require that $\pi$ is a well-defined policy.
        That is: (a)~$\pi(u,S) := \bot$ whenever $u \in S$, which models invalid input to the policy; (b)~$\pi(u,S) = r$ iff $S = \emptyset$, which models that the salesperson must return to the depot only when there are no other vertices to call; and (c)~$\pi(u,S) \in S$ iff $S \neq \emptyset$, which models that the salesperson must call one of the vertices that have not been called yet. 

        We now describe the (random) tour $T(A;\pi)$ traversed by an adaptive policy $\pi$ when the set of active vertices is $A$.  
        We drop the argument $\pi$ when it is clear from the context. 
        Note that $A$ is unknown to $\pi$. 
        Initially, $u \leftarrow r$ and $S \leftarrow V$, which models that the salesperson starts at the depot vertex and they have not called any vertex so far.  
        As long as $S \neq \emptyset$ or $u \neq r$, the salesperson computes $v = \pi(u,S)$. 
        If $S \neq \emptyset$, the salesperson calls $v$. 
        (Note that $v \in S$ since $\pi$ is well-defined.) 
        If $v \in A$, then the salesperson travels from $u$ to $v$. 
        Otherwise, the salesperson remains at $u$. 
        In the former case, we update $u \leftarrow v$.
        In both cases, we update $S \leftarrow S \setminus \{v\}$.
        If $S = \emptyset$, then $v = r$ holds.
        As all vertices have been called by now, the salesperson simply returns to $r$. 
        This is modeled by setting $u \leftarrow r$.
        
        Observe that $T(A;\pi)$ corresponds to a random tour $(r,v_1,\ldots,v_{|A|},r)$ on $\{r\} \cup A$, where $v_i$ is the $i$th among $A$ to be called. 
        We again overload the cost function and define $c(\pi) := \E_{A \sim \mu}[c(T(A;\pi))]$.   
        
\end{enumerate}

\section{Research Question}
A priori TSP is the most nonadaptive policy, since we cannot adjust our strategy at all.
A posteriori TSP is the most adaptive policy where we can always know which vertices would be active and then make our decision.
Adaptive TSP lies in between, because we are allowed to use the revealed information about the vertices that they have called to adjust the next decision.

We define $\mathcal{I} := (V \cup \{r\}, d, \{p_v\})$ to be an instance of a stochastic TSP then clearly for any instance $\mathcal{I}$ we have:
\begin{equation}\label{eq:optineq}
\optpost \le \optadapt \le \optprior    
\end{equation}
since from left to right we have less adaptivity and information, thus end up getting worse optimal cost.

We define \emph{adaptivity gap} to be $\sup_{\mathcal{I}} \f{\optprior}{\optadapt}$, which measures the worst-case ratio over all instances of problem between the cost of an optimal a priori solution vs. an optimal adaptive policy. We also define \emph{clairvoyance gap} to be $\sup_{\mathcal{I}} \f{\optadapt}{\optpost}$ and \emph{obliviousness gap} to be $\sup_{\mathcal{I}} \f{\optprior}{\optpost}$. The clairvoyance gap measures the benefits that an a posteriori solution can get by having clairvoyance, i.e., knowledge of the future. The obliviousness gap measures the gap between being completely oblivious (nonadaptive) to the realizations vs. the a posteriori solution. We remark that literature on a priori TSP uses the term adaptivity gap for the obliviousness gap that we defined above. We deviate from the terminology used in the literature \cite{Christalla2025} for the sake of clarity. The terminology that we use is more standard in stochastic optimization.

For symmetric metrics, the obliviousness gap is known to be at most $O(1)$; in particular oblivious gap $\le 3$ is known in \cite{Shmoys2008}. 
The same upper bound carries over to clairvoyance gap and adaptivity gap because of Eq~\eqref{eq:optineq}. 
One of the objectives of this project is to obtain better upper and lower bound on clairvoyance gap and adaptivity gap.
% note that the lower bound of prior/post is not clear (to show that prior solution cannot not be very good to some extent). in paper "Improved guarantees for the a priori TSP" we have lower bound for master tour ratio, which means if we use algorithm with this pattern: sample a set, construct best tsp, then connect edges; this scheme will give you no good than 2.6** than optimal a priori in some instance

For asymmetric metrics, Christalla, Puhlmann and Traub \cite{Christalla2025} show that the obliviousness gap is known to be $O(\sqrt{n})$ and $\Omega(n^{1/4} \cdot \log^{-1}n)$. 
Moreover, it is not hard to show that the construction in Section~3.1 in \cite{Christalla2025} also implies that adaptivity gap = $\Omega(n^{1/4} \cdot \log^{-1}n)$. 
Another key objective of my project is to prove or disprove that the clairvoyance gap for asymmetric metrics is $O(1)$. In case of a positive resolution, a further goal would be to design an adaptive policy matching this gap up to constant factors. If not, my aim is to construct an infinite family of instances for which the clairvoyance gap grows to infinity as $n$ goes to infinity.

% Finally, beyond purely analysis questions, we also study algorithmic questions:
% can one compute efficiently an optimal or near-optimal adaptive policy, and what approximation
% guarantees are provable?


\printbibliography

\end{document}



%%67 opt post <= opt adapt <= opt nonadapt
% for symmetric d known that nonadapt / post <= O(1);  also mention the lower boundthis directly implies that opt adapt / opt post & opt nonadapt / opt adapt <= O(1) *also whats the lower bound for both
% nevertheless we interested in showing better bound
%%
% asymmetric, known that opt nonadapt / opt post is >= n ^1/4, see section blah
% It is not hard to see that the same example implies opt nonadapt / opt adapt >- n^1/4 questions 
% question1 is opt adapt  / opt post = O(1) for all instances? or does it grow to inf with n; can we construct a family of instances to achieve this behaviour
% question2 can we effienctly compute a optimal/near optimal policy
%%